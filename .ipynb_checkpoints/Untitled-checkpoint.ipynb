{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "significant-turner",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data  = pd.read_csv(\"URS-master/scrapes/04-01-2021/r-all-Search-'abortion'-past-year.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stretch-wisconsin",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"URL\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ultimate-trust",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_source(x):\n",
    "    x = x.split(\"/\")\n",
    "    if x[3] == \"r\":\n",
    "        return x[4]\n",
    "    elif \"redd.it\" in x[2] or \"imgur.com\" in x[2]:\n",
    "        return \n",
    "    elif \"twitter.com\" in x[2]:\n",
    "        return \"twitter.com/{}\".format(x[3])\n",
    "    else:\n",
    "        return x[2]\n",
    "\n",
    "data[\"URL\"].apply(lambda x : get_source(x)).unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "processed-north",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"AFaAUdloxpfo7w\",\n",
    "    client_secret=\"pfpsSSWvRcD1kT25MmeGnN_oB5n4Zg\",\n",
    "    user_agent=\"personal use script\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "large-receptor",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_and_levels = []\n",
    "\n",
    "for post_id in data[\"ID\"]:\n",
    "    post = reddit.submission(id=post_id)\n",
    "    \n",
    "    #Parse an entire reddit post and grab its level, upvotes, and id\n",
    "    def parse_comments(top_comment, level):\n",
    "        body = top_comment.body\n",
    "        score = top_comment.score\n",
    "        id_ = top_comment.id\n",
    "        #Only grabbing cogent sentences, lets say if length is > 100\n",
    "        #Also splitting by linebreak - as well formed ideas are usually broken into sections\n",
    "        if len(body) >= 100:\n",
    "            cleanup = top_comment.body.split(\"\\n\")\n",
    "            ideas = []\n",
    "            for x in cleanup:\n",
    "                if len(x) > 20:\n",
    "                    ideas.append(x)\n",
    "            comments_and_levels.append((ideas, level, score, id_, post_id))\n",
    "        for comment in top_comment.replies:\n",
    "            parse_comments(comment, level+1)\n",
    "\n",
    "    post.comments.replace_more(limit=0)\n",
    "    comments_and_levels.append((post.title, 0, post.score, post_id, post_id))\n",
    "\n",
    "    for comment in post.comments:\n",
    "        parse_comments(comment, 1)\n",
    "    \n",
    "data = pd.DataFrame(comments_and_levels, columns=[\"text\", \"level\", \"score\", \"id\", \"parent\"])\n",
    "data = data.explode(\"text\")\n",
    "data = data.drop_duplicates(subset=\"text\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authentic-standing",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"abortion.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "tough-guess",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>processed text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You’re either a dedicated subservient housewif...</td>\n",
       "      <td>either dedicated subservient housewife hoe rap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>She keeps flip flopping between \"look I'm an o...</td>\n",
       "      <td>She keeps flip flopping look Im ok looking wom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Anyway she's an actual fascist and everyone sh...</td>\n",
       "      <td>Anyway shes actual fascist everyone keep mind</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>With that said, here's a quote - What is fascism?</td>\n",
       "      <td>With said heres quote What fascism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&gt;[Once more, let me remind you what fascism is...</td>\n",
       "      <td>Once let remind fascism need wear brown shirt ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28810</th>\n",
       "      <td>Calling her a sinner and saying maybe if she a...</td>\n",
       "      <td>Calling sinner saying maybe asks forgiveness b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28811</th>\n",
       "      <td>You masquerade as a \"good Godly person\" and th...</td>\n",
       "      <td>You masquerade good Godly person say things li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28812</th>\n",
       "      <td>Did you read the post yes it does have to do w...</td>\n",
       "      <td>Did read post yes PCOS PCOS got abortion scare...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28813</th>\n",
       "      <td>I don’t think this is helpful in the slightest...</td>\n",
       "      <td>think helpful slightest Of course google PCOSa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28814</th>\n",
       "      <td>Before posting, stop and wonder “is what I hav...</td>\n",
       "      <td>Before posting stop wonder say</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28815 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  \\\n",
       "0      You’re either a dedicated subservient housewif...   \n",
       "1      She keeps flip flopping between \"look I'm an o...   \n",
       "2      Anyway she's an actual fascist and everyone sh...   \n",
       "3      With that said, here's a quote - What is fascism?   \n",
       "4      >[Once more, let me remind you what fascism is...   \n",
       "...                                                  ...   \n",
       "28810  Calling her a sinner and saying maybe if she a...   \n",
       "28811  You masquerade as a \"good Godly person\" and th...   \n",
       "28812  Did you read the post yes it does have to do w...   \n",
       "28813  I don’t think this is helpful in the slightest...   \n",
       "28814  Before posting, stop and wonder “is what I hav...   \n",
       "\n",
       "                                          processed text  \n",
       "0      either dedicated subservient housewife hoe rap...  \n",
       "1      She keeps flip flopping look Im ok looking wom...  \n",
       "2          Anyway shes actual fascist everyone keep mind  \n",
       "3                     With said heres quote What fascism  \n",
       "4      Once let remind fascism need wear brown shirt ...  \n",
       "...                                                  ...  \n",
       "28810  Calling sinner saying maybe asks forgiveness b...  \n",
       "28811  You masquerade good Godly person say things li...  \n",
       "28812  Did read post yes PCOS PCOS got abortion scare...  \n",
       "28813  think helpful slightest Of course google PCOSa...  \n",
       "28814                     Before posting stop wonder say  \n",
       "\n",
       "[28815 rows x 2 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "data = pd.read_csv(\"abortion.csv\")\n",
    "#Getting comments\n",
    "\n",
    "comments = []\n",
    "for comment in data[\"text\"]:\n",
    "    comments.append(str(comment))\n",
    "comments\n",
    "\n",
    "\n",
    "def prep(row):     \n",
    "    # split into tokens by white space\n",
    "    tokens = row.split()\n",
    "    # prepare regex for char filtering\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    # remove punctuation from each word\n",
    "    tokens = [re_punc.sub('', w) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if w not in stop_words]\n",
    "    # stemming of words\n",
    "    porter = PorterStemmer()\n",
    "    stemmed = [porter.stem(word) for word in tokens]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "comments = pd.DataFrame(comments,columns=[\"text\"])\n",
    "comments[\"processed text\"] = comments[\"text\"].apply(lambda x: prep(x))\n",
    "comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fourth-timing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start tfidf\n",
      "finished tfidf\n",
      "starting BERT\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n"
     ]
    }
   ],
   "source": [
    "#Using BERT and TFIDF\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "import pandas as pd\n",
    "model = SentenceTransformer('bert-large-uncased')\n",
    "\n",
    "bert_embeddings = []\n",
    "v = TfidfVectorizer()\n",
    "print(\"start tfidf\")\n",
    "x = v.fit_transform(comments[\"processed text\"])\n",
    "print(\"finished tfidf\")\n",
    "print(\"starting BERT\")\n",
    "i = 0\n",
    "for comment in comments[\"processed text\"]:\n",
    "    i += 1\n",
    "    bert_embeddings.append(model.encode(comment))\n",
    "    if i % 1000 == 0:\n",
    "        print(i)\n",
    "bert_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beneficial-dinner",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"abortion_bert.txt\", \"w\") as f:\n",
    "    f.write(str(bert_embeddings))\n",
    "    \n",
    "with open(\"abortion_tfidf.txt\", \"w\") as f:\n",
    "    f.write(str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intelligent-density",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatting both embeddings togeteher\n",
    "\n",
    "v = TfidfVectorizer()\n",
    "x = v.fit_transform(comments)\n",
    "tfidf = x.toarray()\n",
    "\n",
    "bert_embeddings = pd.DataFrame(bert_embeddings)\n",
    "tfidf_embeddings = pd.DataFrame(tfidf)\n",
    "out = pd.concat([bert_embeddings, tfidf_embeddings], axis=1)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intelligent-teens",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graphing the embeddings\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "X_embedded = TSNE(n_components=2).fit_transform(out)\n",
    "x, y = zip(*X_embedded)\n",
    "\n",
    "import seaborn as sns\n",
    "graph  = pd.DataFrame([x,y]).transpose()\n",
    "sns.scatterplot(x=graph[0], y=graph[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepted-attitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_look = data[data[\"parent\"] == \"mdam8a\"].index\n",
    "to_look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civic-chuck",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clustering shit totgehter\n",
    "\n",
    "import hdbscan\n",
    "\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=5, gen_min_span_tree=False,min_samples=2)\n",
    "clusterer.fit(tfidf_embeddings[tfidf_embeddings.index.isin(to_look)])\n",
    "clusterer.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "drawn-right",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking if this clustering is better\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "clustering = AgglomerativeClustering(n_clusters=10).fit(out)\n",
    "clustering.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imperial-affairs",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"labels\"] = clusterer.labels_\n",
    "data[data[\"labels\"]==3][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elementary-myanmar",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
